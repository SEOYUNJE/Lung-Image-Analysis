{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/fP59QqEvB75ZeJvpYqR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOYUNJE/Lung-Image-Analysis/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.researchgate.net/publication/368303422/figure/fig1/AS:11431281118333772@1675729425161/CBAM-structure-a-Convolutional-Block-Attention-Module-b-Channel-Attention-Module.png)"
      ],
      "metadata": {
        "id": "cBLp3oU_froq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgLcUtNGc9Fe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import *\n",
        "\n",
        "class CAM(layers.Layer):\n",
        "    def __init__(self, ratio=8):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "        self.gap = layers.GlobalAveragePooling2D()\n",
        "        self.gmp = layers.GlobalMaxPooling2D()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv1 = layers.Conv2D(input_shape[-1]//self.ratio,\n",
        "                                   kernel_size=1,\n",
        "                                   strides=1, padding='same',\n",
        "                                   activation='relu')\n",
        "        self.conv2 = layers.Conv2D(input_shape[-1],\n",
        "                                   kernel_size=1,\n",
        "                                   strides=1, padding='same',\n",
        "                                   activation='relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        gap = self.gap(inputs)\n",
        "        gmp = self.gmp(inputs)\n",
        "        gap = layers.Reshape((1,1,gap.shape[1]))(gap)\n",
        "        gmp = layers.Reshape((1,1,gmp.shape[1]))(gmp)\n",
        "        gap_out = self.conv2(self.conv1(gap))\n",
        "        gmp_out = self.conv2(self.conv1(gmp))\n",
        "\n",
        "        return tf.math.sigmoid(gap_out+gmp_out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SAM(layers.Layer):\n",
        "    def __init__(self, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv2D(64,\n",
        "                                            kernel_size=kernel_size,\n",
        "                                            use_bias=False,\n",
        "                                            kernel_initializer='he_normal',\n",
        "                                            strides=1, padding='same',\n",
        "                                            activation=tf.nn.relu)\n",
        "        self.conv2 = layers.Conv2D(32, kernel_size=kernel_size,\n",
        "                                            use_bias=False,\n",
        "                                            kernel_initializer='he_normal',\n",
        "                                            strides=1, padding='same',\n",
        "                                            activation=tf.nn.relu)\n",
        "        self.conv3 = layers.Conv2D(16, kernel_size=kernel_size,\n",
        "                                            use_bias=False,\n",
        "                                            kernel_initializer='he_normal',\n",
        "                                            strides=1, padding='same',\n",
        "                                            activation=tf.nn.relu)\n",
        "        self.conv4 = layers.Conv2D(1,\n",
        "                                            kernel_size=(1, 1),\n",
        "                                            use_bias=False,\n",
        "                                            kernel_initializer='he_normal',\n",
        "                                            strides=1, padding='same',\n",
        "                                            activation=tf.math.sigmoid)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        avg_out = tf.reduce_mean(inputs, axis=3)\n",
        "        max_out = tf.reduce_max(inputs,  axis=3)\n",
        "        x = tf.stack([avg_out, max_out], axis=3)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        return self.conv4(x)"
      ],
      "metadata": {
        "id": "cCy8FeVqf94_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWeightedAverage2D(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = tf.keras.initializers.get('uniform')\n",
        "        super(AttentionWeightedAverage2D, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [layers.InputSpec(ndim=4)]\n",
        "        assert len(input_shape) == 4\n",
        "        self.W = self.add_weight(shape=(input_shape[3], 1),\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 initializer=self.init)\n",
        "        self._trainable_weights = [self.W]\n",
        "        super(AttentionWeightedAverage2D, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # computes a probability distribution over the timesteps\n",
        "        # uses 'max trick' for numerical stability\n",
        "        # reshape is done to avoid issue with Tensorflow\n",
        "        # and 2-dimensional weights\n",
        "        logits  = K.dot(x, self.W)\n",
        "        x_shape = K.shape(x)\n",
        "        logits  = K.reshape(logits, (x_shape[0], x_shape[1], x_shape[2]))\n",
        "        ai      = K.exp(logits - K.max(logits, axis=[1,2], keepdims=True))\n",
        "\n",
        "        att_weights    = ai / (K.sum(ai, axis=[1,2], keepdims=True) + K.epsilon())\n",
        "        weighted_input = x * K.expand_dims(att_weights)\n",
        "        result         = K.sum(weighted_input, axis=[1,2])\n",
        "        return result"
      ],
      "metadata": {
        "id": "sK0uIoEyhWWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Using Convolutional Block Attention Module(CBAM)**"
      ],
      "metadata": {
        "id": "IrDryjtIjHQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    inp = tf.keras.layers.Input(shape=(256,256,3))\n",
        "\n",
        "    res_model = resnet18(include_top=False, weights='imagenet', input_shape=(256,256,3))\n",
        "\n",
        "    ## Freezing Layers\n",
        "    for layer in res_model.layers[:len(res_model.layers)//10]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Output\n",
        "    x = res_model(inp)\n",
        "    x1 = CAM()(x)\n",
        "    x1 = layers.Multiply()([x,x1])\n",
        "    x2 = SAM()(x1)\n",
        "    x = layers.Multiply()([x1,x2])\n",
        "    x = AttentionWeightedAverage2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(len(TARGET), activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
        "\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy', f1_score])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Sk2oam_Hhtmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
